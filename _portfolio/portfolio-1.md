---
title: "TeenyTinyLlama: open-source, tiny, and totaly Brazilian"
excerpt: "TeenyTinyLlama is an open‑source initiative dedicated to developing compact yet capable foundational language models trained natively in Brazilian Portuguese.<br/><img src='/images/teeny-tiny-llama-logo.png'>"
collection: portfolio
---

TeenyTinyLlama is a lovingly engineered family of tiny-but-mighty open-source language models trained natively in Brazilian Portuguese. Designed for researchers, tinkerers, and anyone exploring LLMs beyond the English-centric world, TeenyTinyLlama shows how far small models can go when optimized for a specific linguistic ecosystem.

Built entirely on open tooling, the project charts the whole journey of developing foundation models for low-resource languages—from tokenizer training to large-scale pre-training, evaluation, and fine-tuning—while keeping the entire pipeline transparent, reproducible, and beautifully lightweight. Despite their size, these models pack a surprising punch, offering a controlled research testbed for studying multilinguality, bias, hallucinations, and efficiency constraints in language modeling.

<div style="text-align: center;">
  <p>
    <a href="https://huggingface.co/collections/nicholasKluge/teenytinyllama-6582ea8129e72d1ea4d384f1" target="_blank" rel="noopener noreferrer">Hugging Face</a>
    <span aria-hidden="true"> | </span>
    <a href="https://arxiv.org/abs/2401.16640" target="_blank" rel="noopener noreferrer">Preprint</a>
    <span aria-hidden="true"> | </span>
    <a href="https://www.sciencedirect.com/science/article/pii/S2666827024000343?via%3Dihub" target="_blank" rel="noopener noreferrer">Paper</a>
    <span aria-hidden="true"> | </span>
    <a href="https://huggingface.co/spaces/nicholasKluge/TeenyTinyLlama-Chat" target="_blank" rel="noopener noreferrer">Demo</a>
  </p>
</div>
